{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "World Level LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdhdgnyBzxt1"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers\n",
        "import tensorflow.keras.utils as ku \n",
        "import numpy as np\n",
        "import re\n",
        "import tensorflow as tf"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ahf5Qakz-GU",
        "outputId": "7bf6b957-e92e-49bc-f9f4-98325b049098"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "data = open('NLP task.txt').read()\n",
        "corpus = data.lower().split(\".\")\n",
        "for i in range(0,len(corpus)):\n",
        "  s = re.sub(' +',' ',(re.sub(r'[^\\w]', ' ', corpus[i])))\n",
        "  corpus[i] = s\n",
        "print(len(corpus))\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6417\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vi8NT8W-0p9w"
      },
      "source": [
        "input_sequences = []\n",
        "for j in corpus:\n",
        " token_list = tokenizer.texts_to_sequences([j])[0]\n",
        " for i in range(1, len(token_list)):\n",
        "  n_gram_sequence = token_list[:i+1]\n",
        "  input_sequences.append(n_gram_sequence)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JW-ibCBj5JYS"
      },
      "source": [
        "# pad sequences \n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLdrl6Fu5Pwn"
      },
      "source": [
        "# create predictors and label\n",
        "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "label = ku.to_categorical(label, num_classes=total_words)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NBPXWIEzv1R",
        "outputId": "cd676dd1-5416-4713-ffd9-bdcf956e553d"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(150, return_sequences = True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(total_words/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 104, 100)          816300    \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 104, 300)          301200    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 104, 300)          0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               160400    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 4081)              412181    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 8163)              33321366  \n",
            "=================================================================\n",
            "Total params: 35,011,447\n",
            "Trainable params: 35,011,447\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ukDhugA9L8J"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "filepath = \"model_training.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss',\n",
        "                             verbose=1, save_best_only=True,\n",
        "                             mode='min')\n",
        "callbacks = [checkpoint]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiDIPvwi6hg0",
        "outputId": "72a74ebf-5ba6-43a1-9f58-be5da8547427"
      },
      "source": [
        "history = model.fit(predictors, label, epochs=500, verbose=1,callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "3207/3207 [==============================] - 148s 35ms/step - loss: 6.6852 - accuracy: 0.0547\n",
            "\n",
            "Epoch 00001: loss improved from inf to 6.43854, saving model to model_training.hdf5\n",
            "Epoch 2/500\n",
            "3207/3207 [==============================] - 117s 37ms/step - loss: 5.9990 - accuracy: 0.0806\n",
            "\n",
            "Epoch 00002: loss improved from 6.43854 to 5.95787, saving model to model_training.hdf5\n",
            "Epoch 3/500\n",
            "3207/3207 [==============================] - 117s 36ms/step - loss: 5.7410 - accuracy: 0.1036\n",
            "\n",
            "Epoch 00003: loss improved from 5.95787 to 5.71359, saving model to model_training.hdf5\n",
            "Epoch 4/500\n",
            "3207/3207 [==============================] - 117s 36ms/step - loss: 5.5272 - accuracy: 0.1199\n",
            "\n",
            "Epoch 00004: loss improved from 5.71359 to 5.51309, saving model to model_training.hdf5\n",
            "Epoch 5/500\n",
            "3207/3207 [==============================] - 118s 37ms/step - loss: 5.3612 - accuracy: 0.1328\n",
            "\n",
            "Epoch 00005: loss improved from 5.51309 to 5.35200, saving model to model_training.hdf5\n",
            "Epoch 6/500\n",
            "3207/3207 [==============================] - 116s 36ms/step - loss: 5.2126 - accuracy: 0.1433\n",
            "\n",
            "Epoch 00006: loss improved from 5.35200 to 5.22507, saving model to model_training.hdf5\n",
            "Epoch 7/500\n",
            "3207/3207 [==============================] - 116s 36ms/step - loss: 5.1127 - accuracy: 0.1493\n",
            "\n",
            "Epoch 00007: loss improved from 5.22507 to 5.12018, saving model to model_training.hdf5\n",
            "Epoch 8/500\n",
            "3207/3207 [==============================] - 118s 37ms/step - loss: 5.0164 - accuracy: 0.1539\n",
            "\n",
            "Epoch 00008: loss improved from 5.12018 to 5.02884, saving model to model_training.hdf5\n",
            "Epoch 9/500\n",
            "3207/3207 [==============================] - 118s 37ms/step - loss: 4.9428 - accuracy: 0.1581\n",
            "\n",
            "Epoch 00009: loss improved from 5.02884 to 4.94923, saving model to model_training.hdf5\n",
            "Epoch 10/500\n",
            "3207/3207 [==============================] - 117s 37ms/step - loss: 4.8539 - accuracy: 0.1638\n",
            "\n",
            "Epoch 00010: loss improved from 4.94923 to 4.87624, saving model to model_training.hdf5\n",
            "Epoch 11/500\n",
            "3207/3207 [==============================] - 117s 37ms/step - loss: 4.7919 - accuracy: 0.1669\n",
            "\n",
            "Epoch 00011: loss improved from 4.87624 to 4.80750, saving model to model_training.hdf5\n",
            "Epoch 12/500\n",
            "3207/3207 [==============================] - 117s 37ms/step - loss: 4.7267 - accuracy: 0.1710\n",
            "\n",
            "Epoch 00012: loss improved from 4.80750 to 4.74518, saving model to model_training.hdf5\n",
            "Epoch 13/500\n",
            "3207/3207 [==============================] - 117s 37ms/step - loss: 4.6579 - accuracy: 0.1771\n",
            "\n",
            "Epoch 00013: loss improved from 4.74518 to 4.68491, saving model to model_training.hdf5\n",
            "Epoch 14/500\n",
            "3207/3207 [==============================] - 116s 36ms/step - loss: 4.6023 - accuracy: 0.1819\n",
            "\n",
            "Epoch 00014: loss improved from 4.68491 to 4.62998, saving model to model_training.hdf5\n",
            "Epoch 15/500\n",
            "3207/3207 [==============================] - 119s 37ms/step - loss: 4.5419 - accuracy: 0.1869\n",
            "\n",
            "Epoch 00015: loss improved from 4.62998 to 4.57611, saving model to model_training.hdf5\n",
            "Epoch 16/500\n",
            "3207/3207 [==============================] - 124s 39ms/step - loss: 4.5126 - accuracy: 0.1877\n",
            "\n",
            "Epoch 00016: loss improved from 4.57611 to 4.52433, saving model to model_training.hdf5\n",
            "Epoch 17/500\n",
            "3207/3207 [==============================] - 122s 38ms/step - loss: 4.4514 - accuracy: 0.1930\n",
            "\n",
            "Epoch 00017: loss improved from 4.52433 to 4.47394, saving model to model_training.hdf5\n",
            "Epoch 18/500\n",
            "3207/3207 [==============================] - 121s 38ms/step - loss: 4.3853 - accuracy: 0.1990\n",
            "\n",
            "Epoch 00018: loss improved from 4.47394 to 4.42806, saving model to model_training.hdf5\n",
            "Epoch 19/500\n",
            " 435/3207 [===>..........................] - ETA: 1:43 - loss: 4.3386 - accuracy: 0.2071"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swsV26fWJrE0"
      },
      "source": [
        "model.save(\"model1.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdQggwItJ3fa"
      },
      "source": [
        "seed_text = \"sherlock was always\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        " token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        " token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        " predicted = model.predict_classes(token_list, verbose=0)\n",
        " output_word = \"\"\n",
        " for word, index in tokenizer.word_index.items():\n",
        "  if index == predicted:\n",
        "   output_word = word\n",
        "   break\n",
        " seed_text += \" \" + output_word\n",
        "print(seed_text)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}